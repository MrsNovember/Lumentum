pub fn tokenize(text: &str) -> Vec<String> {
    // Basic whitespace split but keeping Unicode in mind
    // In a more advanced version, we could use regex or a specialized library
    text.split_whitespace()
        .map(|s| s.to_string())
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_tokenize_basic() {
        let tokens = tokenize("Lumentum hÄ±zÄ± artÄ±rÄ±r.");
        assert_eq!(tokens, vec!["Lumentum", "hÄ±zÄ±", "artÄ±rÄ±r."]);
    }

    #[test]
    fn test_tokenize_unicode() {
        let tokens = tokenize("GÃ¶z yorgunluÄŸu azalÄ±r ğŸ‘ï¸");
        assert_eq!(tokens, vec!["GÃ¶z", "yorgunluÄŸu", "azalÄ±r", "ğŸ‘ï¸"]);
    }
}
